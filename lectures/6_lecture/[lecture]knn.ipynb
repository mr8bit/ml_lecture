{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Метод ближайших соседей\n",
    "\n",
    "Метод ближайших соседей (k Nearest Neighbors, или kNN) — тоже очень популярный метод классификации, также иногда используемый в задачах регрессии. Это, наравне с деревом решений, один из самых понятных подходов к классификации. На уровне интуиции суть метода такова: посмотри на соседей, какие преобладают, таков и ты. Формально основой метода является гипотезой компактности: если метрика расстояния между примерами введена достаточно удачно, то схожие примеры гораздо чаще лежат в одном классе, чем в разных.\n",
    "\n",
    "Согласно методу ближайших соседей, тестовый пример (зеленый шарик) будет отнесен к классу \"синие\", а не \"красные\".\n",
    "\n",
    "![](https://mlcourse.ai/notebooks/blob/master/img/topic3_knn_intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если не знаешь, какой тип товара указать в объявлении для Bluetooth-гарнитуры, можешь найти 5 похожих гарнитур, и если 4 из них отнесены к категории \"Аксессуары\", и только один - к категории \"Техника\", то здравый смысл подскажет для своего объявления тоже указать категорию \"Аксессуары\".\n",
    "\n",
    "Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:\n",
    "\n",
    "- Вычислить расстояние до каждого из объектов обучающей выборки\n",
    "- Отобрать k\n",
    "\n",
    "объектов обучающей выборки, расстояние до которых минимально\n",
    "\n",
    "![](img/knn.png)\n",
    "\n",
    "Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k\n",
    "- ближайших соседей\n",
    "- Примечательное свойство такого подхода – его ленивость. Это значит, что вычисления начинаются только в момент классификации тестового примера, а заранее, только при наличии обучающих примеров, никакая модель не строится. В этом отличие, например, от ранее рассмотренного дерева решений, где сначала на основе обучающей выборки строится дерево, а потом относительно быстро происходит классификация тестовых примеров.\n",
    "\n",
    "Стоит отметить, что метод ближайших соседей – хорошо изученный подход (в машинном обучении, эконометрике и статистике больше известно наверно только про линейную регрессию). Для метода ближайших соседей существует немало важных теорем, утверждающих, что на \"бесконечных\" выборках это оптимальный метод классификации. Авторы классической книги \"The Elements of Statistical Learning\" считают kNN теоретически идеальным алгоритмом, применимость которого просто ограничена вычислительными возможностями и проклятием размерностей.\n",
    "Метод ближайших соседей в реальных задачах\n",
    "\n",
    "- В чистом виде kNN может послужить хорошим стартом (baseline) в решении какой-либо задачи;\n",
    "- В соревнованиях Kaggle kNN часто используется для построения мета-признаков (прогноз kNN подается на вход прочим моделям) или в стекинге/блендинге;\n",
    "- Идея ближайшего соседа расширяется и на другие задачи, например, в рекомендательных системах простым начальным решением может быть рекомендация какого-то товара (или услуги), популярного среди ближайших соседей человека, которому хотим сделать рекомендацию;\n",
    "- На практике для больших выборок часто пользуются приближенными методами поиска ближайших соседей. Вот лекция Артема Бабенко про эффективные алгоритмы поиска ближайших соседей среди миллиардов объектов в пространствах высокой размерности (поиск по картинкам). Также известны открытые библиотеки, в которых реализованы такие алгоритмы, спасибо компании Spotify за ее библиотеку Annoy.\n",
    "\n",
    "Качество классификации методом ближайших соседей зависит от нескольких параметров:\n",
    "\n",
    "- число соседей\n",
    "- метрика расстояния между объектами (часто используются метрика Хэмминга, евклидово расстояние, косинусное расстояние и расстояние Минковского). Отметим, что при использовании большинства метрик значения признаков надо масштабировать. Условно говоря, чтобы признак \"Зарплата\" с диапазоном значений до 100 тысяч не вносил больший вклад в расстояние, чем \"Возраст\" со значениями до 100.\n",
    "- веса соседей (соседи тестового примера могут входить с разными весами, например, чем дальше пример, тем с меньшим коэффициентом учитывается его \"голос\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Центроидный классификатор\n",
    "Еще один простой метрический классификатор — центроидный классификатор. Сначала по обучающей вы-\n",
    "борке $ {(x_i, y_i)}^m_{i=1}$ определяются \"центры\" всех классов ($l_y$ - количество объектов класса y):\n",
    "\n",
    "$$ \\mu = \\frac{1}{l_y} \\sum \\limits_{i:y_i=y}x_i $$\n",
    "\n",
    "\n",
    "![](img/cenroid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого центроидный классификатор относит каждый новый объект $x$ к тому классу, центр которого\n",
    "находится ближе всего в пространстве признаков к признаковому описанию нового объекта:\n",
    "$$ a(x)  = argmin_{y \\in Y} d(\\mu_y, x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Веса соседей как функция от номера\n",
    " Если используется взвешенный kNN, то необходимо задать весовую функцию. Такая функция не должна\n",
    "возрастать с ростом номера объекта. Простейший вариант — это $w(x) = 1$ . При выборе весовой функции всегда следует сначала пробовать его перед тем, как рассматривать более сложные варианты.\n",
    "\n",
    "Если выбор $w(x)= 1$ не дает желаемых результатов, можно попробовать определить веса как функцию\n",
    "от номера соседа:\n",
    "\n",
    "\n",
    "- $w(i)= q^i$, $0<q<1$\n",
    "- $w(i)= \\frac{1}{i}$,$ w(i)= \\frac{1}{i+a}$, $w(i)=\\frac{1}{(i+a)^b}$ \n",
    "- $w(i) = 1 - \\frac{i-1}{k}$\n",
    "\n",
    "\n",
    "Функция, которая линейно зависит от номера соседа, не является хорошим выбором. Например, в случае\n",
    "$k = 4$ , если 1 и 4 соседи некоторого объектаx принадлежат к первому классу, а 2 и 3 — ко второму, алгоритм\n",
    "не сможет классифицировать этот объект. Это, конечно, не значит, что эта функция вовсе не применима на\n",
    "практике, но эту её особенность следует учитывать.\n",
    "\n",
    "\n",
    "## Веса объектов как функция от расстояния\n",
    "\n",
    "Другой способ определить весовую функцию — задать ее как функцию от расстояния. Ранее уже было сказа-\n",
    "но, что в задаче регрессии выбор весовой функции $w(d)= \\frac{1}{d}$ приводит к переобученности. Грубо говоря, это было связано с тем, что при $d = 0$ начение весовой функции было бесконечно большим. Поэтому необходимо\n",
    "более аккуратно выбирать весовую функцию. Например, можно попробовать следующие варианты:\n",
    "\n",
    "- $w(d) = \\frac{1}{(d+a)^b)}$\n",
    "- $w(d) = q^d$, $0<q<1$\n",
    "\n",
    "Существует более общий подход к придумыванию функции весов, зависящих от расстояний, который основан\n",
    "на использовании так называемых ядер. Но этот подход сейчас не будет обсуждаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понятие метрики\n",
    "Метрика является функцией, задающей расстояние в метрическом пространстве, и должна удовлетворять\n",
    "следующим аксиомам:\n",
    "- p(x,y) >=0, причем p(x,y) = 0 x=y\n",
    "- p(x,y)  = p(y,x)\n",
    "- p(x,y) =< p(x,z) + p(z,y)\n",
    "\n",
    "\n",
    "Эти аксиомы не будут обсуждаться в рамках данного раздела, так как излагаемый далее материал носит\n",
    "исключительно прикладной характер.\n",
    "Можно привести следующие примеры метрик:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции близости\n",
    "\n",
    "Часто, особенно в задачах анализа текста, используется так называема косинусная мера, которая представляет сообой косинус угла между векторами:\n",
    "\n",
    "$$ \\text{similarity} = cos(\\theta) = \\frac{x*y}{||x|| * ||y||} = \\frac{\\sum \\limits^n_{i=1} x_i y_i}{\\sqrt{\\sum \\limits^n_{i=1} x_i^2} \\sqrt{ \\sum \\limits^n_{i=1} y_i^2}}  $$\n",
    "\n",
    "\n",
    "Причем косинусная мера — не расстояние, а функция близости, то есть такая функция, которая тем больше,\n",
    "чем больше объекты друг на друга похожи.\n",
    "В рекомендательных системах используется коэффициент корреляции r. Он также может быть использован как функция близости и похож на косинусную меру:\n",
    "\n",
    "$$ r = \\frac{\\sum \\limits^n_{i=1} ((x_i - \\dot{x}) (y_i - \\dot{y}))}{\\sqrt{\\sum \\limits^n_{i=1} (x_i - \\dot{x}})^2 {\\sqrt{\\sum \\limits^n_{i=1} (y_i - \\dot{y})}}}  $$ \n",
    "\n",
    "\n",
    "Конечно, существует и много других функций близости, которые в разной степени учитывают разные раз-\n",
    "личия между векторами:\n",
    "- Скалярное произведение - $ \\sum x_i y_i$\n",
    "- Коэффициент Дайса - $ \\frac{2\\sum{x_i y_i}}{ \\sum{x_i^2}  \\sum{y_i^2}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
